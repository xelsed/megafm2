Great, I’ll dive into current techniques used in both deterministic and non-deterministic algorithmic music generation—including neural networks, fractal and Euclidean approaches, and other algorithmic strategies. I’ll also research how this kind of generative music is visualized in real time, especially for MIDI-driven systems, with a focus on expressive, accessible 3D visual representation.

I’ll get back to you with a detailed overview shortly.

# Algorithmic Music Generation and Visualization: A Comprehensive Overview

## Introduction  
Algorithmic music generation refers to using formal procedures or algorithms to create music automatically ([Algorithmic composition - Wikipedia](https://en.wikipedia.org/wiki/Algorithmic_composition#:~:text=Algorithms%20,generating%20techniques%20that)). These algorithms range from **deterministic** methods (which produce the same result given the same input) to **non-deterministic** or stochastic methods (which incorporate randomness or learning to yield varied outcomes). In recent years, modern AI techniques have greatly expanded what is possible in algorithmic composition. Equally important is how we **experience** such music: real-time visualization can make the structure of music perceptible, which is especially valuable for deaf or hard-of-hearing individuals. This report surveys both traditional and modern algorithmic composition techniques (fractal-based generation, Euclidean rhythms, cellular automata, rule-based systems, neural networks like RNNs, Transformers, GANs, etc.) and explores methods for real-time **3D visualization** of music with an emphasis on accessibility and expressive visual metaphors for musical elements.

## Traditional Algorithmic Composition Techniques (Deterministic & Stochastic)  

Historically, composers and researchers have employed a variety of algorithmic techniques to generate music. Many of these are deterministic rule-based systems or mathematical models, though randomness is often introduced to add variation. Table 1 summarizes key traditional techniques:

**Table 1. Traditional Algorithmic Music Generation Methods**  

| Technique                | Method Summary & Characteristics                                             | Example Uses/Systems                 |
|--------------------------|-----------------------------------------------------------------------------|--------------------------------------|
| **Fractal-Based Generation**  | Uses fractal algorithms (self-similar, recursive processes) to produce musical material. Musical parameters (pitch, rhythm, dynamics, etc.) are mapped from fractal patterns. Often yields self-similar structures (repeating patterns at multiple scales). Can generate infinitely varying music by iterating fractal formulas. | *Examples:* Mapping the Mandelbrot set or L-systems to notes ([Algorithmic Composition](https://junshern.github.io/algorithmic-music-tutorial/part1.html#:~:text=example%20we%20will%20focus%20on,System%20will%20require%20three%20things)); software like **FractMus** (Windows fractal music generator). Composers have likened mensuration canons to fractal scaling in early music ([The Dawn of Fractal Music | Harlan Brothers | Science Spectrum](https://medium.com/science-spectrum/the-dawn-of-fractal-music-f0228a8cb4f#:~:text=It%20turns%20out%20that%20musicians,link%20to%20%E2%80%9Cfundamental%20requirements%E2%80%9D%20above)). |
| **Euclidean Rhythms**        | Uses the Euclidean algorithm to evenly distribute a number of beats across a given number of steps. Produces cyclic beat patterns that often match traditional rhythms (e.g. African and Balkan rhythms). Deterministic for given inputs (number of pulses and steps) ([Euclidean rhythm - Wikipedia](https://en.wikipedia.org/wiki/Euclidean_rhythm#:~:text=The%20Euclidean%20rhythm%20in%20music,number%20of%20beats%20and%20silences)). The algorithm finds the most evenly spaced pattern of beats, which Toussaint showed corresponds to many world music rhythms. | *Examples:* Euclidean rhythms are used in electronic sequencers and generative drum machines. Many modern MIDI sequencers or tools have a “Euclidean rhythm” feature for beat pattern generation. Originally described in a 2005 paper “The Euclidean Algorithm Generates Traditional Musical Rhythms” ([Euclidean rhythm - Wikipedia](https://en.wikipedia.org/wiki/Euclidean_rhythm#:~:text=The%20Euclidean%20rhythm%20in%20music,number%20of%20beats%20and%20silences)). |
| **Cellular Automata (CA)**   | Uses cellular automaton grids (e.g. Conway’s Game of Life or 1D binary rule systems) as generative engines. The evolving state of the cellular automaton is mapped to musical parameters (such as interpreting each cell row as a sequence of notes or chords). Starting from an initial seed, the CA’s deterministic rules produce complex, emergent patterns. This can yield unexpected rhythmic or melodic developments, especially if the initial state is randomized. Researchers noted that CA rules can be “translated (mapped) into a music representation” to generate compositions ([](https://isea-archives.siggraph.org/wp-content/uploads/2019/09/1993_Miranda_Cellular_Automata_Music_Composition.pdf#:~:text=that%20music%20researchers%20started%20to,In%20the%20following%20paragraphs%20we)). | *Examples:* **CAMUS** (Cellular Automata MUSic) by E.R. Miranda in the 1990s, which mapped Conway’s Game of Life patterns to melodies ([](https://isea-archives.siggraph.org/wp-content/uploads/2019/09/1993_Miranda_Cellular_Automata_Music_Composition.pdf#:~:text=This%20paper%20introduces%20an%20experimental,aspects%20through%20an%20example%20composition)). Various experimental pieces use Game of Life to trigger notes (alive cell = play a note). Some algorithmic composers use 1D CA (like Rule 90 or 30) to generate drum patterns or tonal sequences. |
| **Rule-Based & Grammar Systems** | Uses explicit compositional rules or grammars to generate music. This spans techniques like formal grammars (L-systems are a type of grammar also used to create self-similar musical structures), as well as AI expert systems encoding music theory rules. For example, the rules of counterpoint or harmony can be programmed so the system generates only combinations that obey those rules. Such systems are typically deterministic if rules are strictly applied, but randomness can be added in rule selection or initial conditions. **Markov chains** and other statistical models also fall here: they use probabilities learned from existing music to generate new sequences (introducing stochasticity). | *Examples:* Mozart’s **Musikalisches Würfelspiel** (Musical Dice Game) in the 18th century used dice rolls to select pre-composed fragments – an early rule+chance system. In the 1990s, **SSEYO Koan** Pro (used by Brian Eno for *Generative Music 1*) let users define musical rules and ranges for generative ambient music. More recently, David Cope’s Experiments in Musical Intelligence (EMI) used rule-based pattern recombination. Many algorithmic composition libraries (e.g. **Csound**, **OpenMusic**, **Max/MSP**) allow rule-based generation. Notably, algorithmic composition has deep roots – even the formal procedures of Western counterpoint can be seen as algorithmic rules ([Algorithmic composition - Wikipedia](https://en.wikipedia.org/wiki/Algorithmic_composition#:~:text=Algorithms%20,generating%20techniques%20that)). |

As Table 1 suggests, traditional methods blend mathematical processes and music theory. Some additional notes:

- **Fractals:** In practice, fractal music generators often map fractal geometry or chaotic attractors to notes. For instance, an algorithm might interpret the iterative values of the Mandelbrot set as MIDI pitches and durations. This tends to produce music with recursive or self-similar motifs. One definition describes *fractal music* broadly as *“an area of algorithmic composition”* where fractal algorithms are applied to determine pitches, rhythms, dynamics, etc.. A famous early example is the self-similar structure of some *mensuration canons* (medieval canons where a melody is repeated at different speeds – a concept analogous to fractal scaling ([The Dawn of Fractal Music | Harlan Brothers | Science Spectrum](https://medium.com/science-spectrum/the-dawn-of-fractal-music-f0228a8cb4f#:~:text=It%20turns%20out%20that%20musicians,link%20to%20%E2%80%9Cfundamental%20requirements%E2%80%9D%20above))).

- **Euclidean Rhythms:** Discovered by Godfried Toussaint (2004), the Euclidean rhythm algorithm takes two numbers (beats and total steps) and distributes the beats as evenly as possible ([Euclidean rhythm - Wikipedia](https://en.wikipedia.org/wiki/Euclidean_rhythm#:~:text=The%20Euclidean%20rhythm%20in%20music,number%20of%20beats%20and%20silences)). Many common rhythms (e.g. Cuban clave, Arab *masmudi*, etc.) can be generated this way. This method is deterministic and valued for creating looping rhythms that feel “balanced.” It’s widely used in generative drum pattern tools.

- **Cellular Automata:** CA-based composition can produce evolving music that feels organic or nature-inspired. Each generation step can be treated as a new bar or phrase of music. For example, one might have a grid where each row is a different instrument or pitch, and each cell’s state (alive/dead) at each time step determines if that instrument plays. Eduardo Miranda’s *CAMUS* system in 1993 demonstrated mapping 2D automata to musical structures ([](https://isea-archives.siggraph.org/wp-content/uploads/2019/09/1993_Miranda_Cellular_Automata_Music_Composition.pdf#:~:text=This%20paper%20introduces%20an%20experimental,aspects%20through%20an%20example%20composition)). Researchers began to suspect that cellular patterns *“could be translated (mapped) into a music representation in order to generate compositional material”* ([](https://isea-archives.siggraph.org/wp-content/uploads/2019/09/1993_Miranda_Cellular_Automata_Music_Composition.pdf#:~:text=that%20music%20researchers%20started%20to,In%20the%20following%20paragraphs%20we)), leading to various approaches where the visual patterns of CA evolution inform rhythms, melodies, and textures.

- **Rule-Based Systems:** Rule-based composition highlights that much of music theory can be codified. For instance, one can program rules to generate four-part chorales by following voice-leading guidelines. These systems can ensure all output meets certain theoretical criteria (e.g. no parallel fifths in counterpoint, or chord progressions following jazz theory). They were precursors to modern AI in that they attempted to encode expert knowledge. A well-known example is the use of **L-systems** (a kind of formal grammar) to generate self-similar musical forms ([Algorithmic composition - Wikipedia](https://en.wikipedia.org/wiki/Algorithmic_composition#:~:text=Some%20algorithms%20or%20data%20that,been%20used%20as%20source%20materials)) – essentially a fractal approach via a production-rule system. In summary, algorithms or even *“formal sets of rules”* have been used to compose music for centuries ([Algorithmic composition - Wikipedia](https://en.wikipedia.org/wiki/Algorithmic_composition#:~:text=Algorithms%20,generating%20techniques%20that)), and today’s rule-based generators are a direct continuation of that idea.

## Modern AI-Based Music Generation (Machine Learning Approaches)  

Modern developments in AI have introduced data-driven, non-deterministic approaches to music generation. Instead of explicitly programming rules, we train models on existing music data so they *learn* musical structures and style. Key techniques include recurrent neural networks (RNNs), Transformers, and generative adversarial networks (GANs), among others (like Variational Autoencoders and Diffusion models). Table 2 gives an overview:

**Table 2. AI Techniques for Music Generation**  

| Approach                     | How It Works and Features                                                             | Notable Projects/Systems                        |
|------------------------------|---------------------------------------------------------------------------------------|-------------------------------------------------|
| **Recurrent Neural Networks (RNNs)**  | Sequence models (often using LSTM or GRU units) that learn to predict the next musical event given previous events, one step at a time. Trained on sequences of notes (e.g. MIDI melodies), they capture local musical structure like melodies and short-term chord progressions. RNNs maintain a hidden state carrying context, but vanilla RNNs have limited memory for long sequences. They are probabilistic models: each run can produce a new variation. RNNs were among the first successful deep learning models for music generation. | *Examples:* **Magenta’s MelodyRNN and DrumRNN** (from Google) which generate melodies or drum patterns in various styles ([Hands-On Music Generation with Magenta - Amazon.com](https://www.amazon.com/Hands-Music-Generation-Magenta-composition/dp/1838824413#:~:text=Hands,for%20generation%20of%20raw%20audio)). **DeepBach** (Gaetan Hadjeres et al.) which uses an LSTM-based approach to harmonize chorales in Bach’s style. Early experiments by Eck & Schmidhuber (2002) trained an LSTM to improvise blues solos. RNN-based systems can output MIDI; e.g., **Performance RNN** generates expressive piano performances event-by-event. |
| **Transformer Networks**     | Deep attention-based sequence models that can capture long-term dependencies and structures in music. The Transformer (Vaswani et al. 2017) uses self-attention mechanisms instead of recurrence, allowing the model to consider the entire sequence context at each step. This enables learning of musical structure at multiple timescales – from motifs to overall form – far better than RNNs. Transformers have proven capable of generating longer and more coherent pieces. They are typically trained on large datasets of MIDI or symbolic music by predicting the next token (note event) in the sequence, similar to how GPT-2/3 are trained on text. | *Examples:* **Music Transformer** (Google Magenta, 2018) introduced relative self-attention to handle music’s repetitive timing, achieving *“improved long-term coherence”* in generated piano music ([Music Transformer: Generating Music with Long-Term Structure](https://magenta.tensorflow.org/music-transformer#:~:text=Generating%20long%20pieces%20of%20music,performances%20generated%20by%20the%20model)). **OpenAI’s MuseNet** (2019) is a 72-million-parameter Transformer that learned from hundreds of thousands of MIDI files; it can generate 4-minute compositions with up to 10 instruments, blending styles from classical to jazz ([MuseNet | OpenAI](https://openai.com/index/musenet/#:~:text=We%E2%80%99ve%20created%20MuseNet%2C%20a%20deep,a%20sequence%2C%20whether%20audio%20or%C2%A0text)). MuseNet discovered musical patterns of harmony and rhythm by training to predict next notes, using the same technology as GPT-2 (a large-scale Transformer) ([MuseNet | OpenAI](https://openai.com/index/musenet/#:~:text=We%E2%80%99ve%20created%20MuseNet%2C%20a%20deep,a%20sequence%2C%20whether%20audio%20or%C2%A0text)). More recently, Transformers underlie models like **GPT-4-based music aides** and **MusicLM** (which generates audio from text). |
| **Generative Adversarial Networks (GANs)** | A framework with two neural nets in competition: a *generator* that tries to create plausible music, and a *discriminator* that evaluates whether the generated output looks like real music. Through training, the generator learns to fool the discriminator, thereby producing increasingly realistic outputs. GANs have been used more in **symbolic multi-track music generation** and in **raw audio synthesis**. They can be harder to train for music (due to sequence coherence issues), but some successes exist. | *Examples:* **MuseGAN** (Dong et al. 2017) applied GANs to symbolic music: it generated multi-track piano-roll representations for band music (drums, bass, piano, etc.), allowing the model to output coherent harmonies across instruments ([MuseGAN: Multi-track Sequential Generative Adversarial Networks ...](https://arxiv.org/abs/1709.06298#:~:text=,GANs)). In the audio domain, GAN variants (like GANSynth) have produced realistic instrument sounds. GANs are also used for *style transfer* in music (making one melody play in the style of another). |
| **Other AI Approaches**      | **Variational Autoencoders (VAE):** learn a latent space of music patterns and enable generating new music by sampling that space (e.g. Google’s *MusicVAE* can interpolate between melodies). **Flow-based and Diffusion models:** latest techniques where models learn to progressively generate or refine music or audio (e.g. *Riffusion* uses a diffusion model on spectrogram images to generate audio). **Reinforcement Learning:** used in some research to have agents learn to compose music that maximizes a reward (e.g. pleasing chords). These are emerging areas building on the symbolic capabilities of RNN/Transformer or the raw audio prowess of models like WaveNet. | *Examples:* **MusicVAE** (Magenta) can interpolate between two melodies or drum beats, creating variations. **OpenAI Jukebox** (2020) used a VAE and Transformer to generate raw audio songs with vocals (though not MIDI, it’s noteworthy as an AI music milestone). **AIVA** (Artificial Intelligence Virtual Artist, a commercial product) employs deep learning to compose soundtrack music; it’s known as an AI composer assisting human creators. **Amper Music** and **Orb Composer** are other products using AI (some likely combining rule-based and learned models). These tools often output MIDI or audio that users can edit. |

Modern AI models are generally **non-deterministic** – they can produce new variations each time (especially if they sample from probability distributions of possible notes). However, they can be guided by training data and conditioning. For example, MuseNet can be prompted with a genre or a few starting measures to steer the style of the composition ([MuseNet | OpenAI](https://openai.com/index/musenet/#:~:text=We%E2%80%99ve%20created%20MuseNet%2C%20a%20deep,a%20sequence%2C%20whether%20audio%20or%C2%A0text)). Likewise, Music Transformer can be primed with an initial motif to elaborate on. The result is that AI generation can blend unpredictability with learned musical idioms.  

It’s important to note that while these AI techniques are powerful, they often require large datasets of existing music and significant computing resources to train. Nonetheless, they have enabled unprecedented achievements like full-length AI-composed albums and interactive music generation tools available to the public. Google’s **Magenta** project provides many open-source examples (RNN-based melody generation, transformer models, VAEs, etc. ([Hands-On Music Generation with Magenta - Amazon.com](https://www.amazon.com/Hands-Music-Generation-Magenta-composition/dp/1838824413#:~:text=Hands,for%20generation%20of%20raw%20audio))). OpenAI’s MuseNet and Jukebox showed that AI can capture complex style nuances. And as these models improve, they are increasingly being integrated into workflows (e.g. as “co-composers” that suggest ideas to human musicians).

## Real-Time 3D Visualization of Algorithmic Music  

Beyond generating music, a key challenge is how to **represent music visually** in real-time. Effective visualization can illuminate the structure of algorithmic music and make the experience accessible to those who cannot hear it. This section explores techniques for visualizing music playback and creation in a 3D space, with an emphasis on being informative and inclusive for deaf or hard-of-hearing users. We will discuss general visualization principles, specific 3D methods, and how musical attributes (pitch, rhythm, harmony, dynamics, etc.) can be mapped to visual metaphors.

### Goals and Principles of Music Visualization  
Music visualization isn’t just about eye-catching graphics; it should convey meaningful information about the music. Research in music information visualization emphasizes grounding the visuals in musical knowledge and human cognitive understanding ([A Survey of Music Visualization Techniques](https://www.audiolabs-erlangen.de/content/05_fau/professor/00_mueller/02_teaching/2024s_sarntal/04_group_VIS/2021_LimaEtAl_MusicVisSurvey_ACM.pdf#:~:text=the%20ancient%20Greeks%20associated%20the,should%20be%20grounded%20on%20both)). In other words, the shapes, colors, and motions should correspond intuitively to musical features. Visualizing music can make it easier to perceive patterns and structures that unfold over time ([A Survey of Music Visualization Techniques](https://www.audiolabs-erlangen.de/content/05_fau/professor/00_mueller/02_teaching/2024s_sarntal/04_group_VIS/2021_LimaEtAl_MusicVisSurvey_ACM.pdf#:~:text=visualize%20similarities%2C%20and%20extract%20more,a%20monolithic%20block%20that%20spans)). For example, repetitive motifs or harmonies might become obvious when seen as recurring visual shapes or positions. According to Isaacson (2013), effective musical visuals must balance **accuracy** (faithfully representing musical data) with **comprehensibility**, avoiding overload. This is even more crucial for deaf/hard-of-hearing users who rely solely on the visual (and possibly tactile) cues for musical information.

Some fundamental design considerations: Why is the visualization needed (analysis, performance, education, accessibility)? What data is being shown (notes, volume, timbre, beat)? And how to represent it (the visual idiom) ([A Survey of Music Visualization Techniques](https://www.audiolabs-erlangen.de/content/05_fau/professor/00_mueller/02_teaching/2024s_sarntal/04_group_VIS/2021_LimaEtAl_MusicVisSurvey_ACM.pdf#:~:text=tempo1%20changes%20are%20often%20derived,proposal%E2%80%99s%20goals%3B%20how%20the%20users)). Two broad types of music visualization are often discussed ([A Survey of Music Visualization Techniques](https://www.audiolabs-erlangen.de/content/05_fau/professor/00_mueller/02_teaching/2024s_sarntal/04_group_VIS/2021_LimaEtAl_MusicVisSurvey_ACM.pdf#:~:text=Music%20visualizations%20can%20be%20divided,both%2C%20augmented%20scores%20and%20performance)): **Augmented scores** (enhancing traditional notation with color or animation for clarity) and **performance visualizations** (showing musical features like volume, pitch, and instruments in intuitive graphics). For real-time algorithmic music, the visualization typically falls in the latter category – a performance visualization that renders note events and expressions as they occur.

For accessibility, studies have found that deaf users prefer visuals that are *“easy to interpret and ‘glance-able’”*, allowing quick understanding at a glance ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=icons%20and%20spectrographs%20that%20can,users%20wanted%20to%20be%20able)). Important requirements include the ability to identify musical events as they happen, to see a history/trail of what just played, and to customize what aspects are shown ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=vicinity,of%20the%20displayed%20sounds%2C%20customize)). These needs inform design choices such as using clear symbols for notes, providing visual timelines or histories, and possibly textual or numeric annotations for clarity. In short, the visualization must transform the abstract temporal experience of music into something spatial and visible, without assuming any auditory input from the user.

### Techniques for 3D Music Visualization  
Traditional audio visualizers (like media player plugins) often show 2D patterns (waveforms, spectra, or abstract animations) reacting to sound. Here we focus on **3D visualization**, which provides an immersive spatial representation. A 3D space can encode multiple musical dimensions simultaneously – for example, using different axes or spatial coordinates for pitch and time, or placing instruments in different locations. The added depth and volume can also create a more engaging experience and potentially map better to how music is performed around an audience (spatialization).

Several approaches to 3D music visualization have been explored:

- **Piano Roll in 3D:** A common method is extending the MIDI piano roll (time vs. pitch grid) into 3D. Time is one axis (often the z-axis going “into” the screen or a circular timeline), pitch is another (y-axis, e.g. vertical), and the visualization shows notes as 3D objects (bars, cubes) at the appropriate time and pitch coordinates. The third axis can be used to show multiple instruments side-by-side or to simply give a sense of depth as notes fly by. *MIDITrail* is an example of this: it’s a MIDI file player that *“provides 3D visualization of MIDI data”*, where multiple tracks are color-coded and the user can navigate the perspective ([MIDI Playback Visualization | University of North Texas](https://digitalstrategy.unt.edu/clear/teaching-resources/accessibility/music-accessibility/midi-playback-visualization.html#:~:text=Many%20different%20options%20MIDI%20realization,help%20further%20augment%20the%20experience)). MIDITrail essentially renders a 3D piano with notes (as beams or boxes) coming towards the viewer (or the player moving through a tunnel of notes). The user can view these “notes” from different angles, making the structure of a piece visually clear.

 ([Summary - MIDITrail](https://www.yknk.org/miditrail/en/summary/)) *Example of a 3D piano-roll style visualization (screenshot from MIDITrail). Here, pitch is mapped to the vertical position aligning with a piano keyboard, time progresses along the depth, and different instrument tracks are shown in different colors. Such visualizations allow the viewer (including those who are hard-of-hearing) to see the melody and harmony unfold in real time.* ([MIDI Playback Visualization | University of North Texas](https://digitalstrategy.unt.edu/clear/teaching-resources/accessibility/music-accessibility/midi-playback-visualization.html#:~:text=Many%20different%20options%20MIDI%20realization,help%20further%20augment%20the%20experience))

- **Geometric Metaphors:** Some systems use more abstract 3D shapes and scenes to represent music. For instance, notes might be represented by shapes (spheres, cubes) whose attributes (size, color, position) encode musical parameters. A classic example by Smith and Williams (1997) mapped each note to a sphere, where *sphere size corresponds to loudness (dynamics), color corresponds to timbre, and vertical position corresponds to pitch*, with different instruments assigned distinct horizontal positions ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=singers,the%20timbre%20of%20the%20tone)). Such a 3D scatter of notes provides a straightforward visual metaphor for the score. Similarly, in a virtual reality context, Valbom and Marcos (2005) *“modeled a 3D environment and represented notes and rhythms with shapes and movements”* ([A Survey of Music Visualization Techniques](https://www.audiolabs-erlangen.de/content/05_fau/professor/00_mueller/02_teaching/2024s_sarntal/04_group_VIS/2021_LimaEtAl_MusicVisSurvey_ACM.pdf#:~:text=%E2%80%9Cthe%20intricate%20body,43)) – for example, a note onset could trigger an object to appear or move, giving a visual pulse for rhythm. In these metaphors, **time** can be represented by movement or animation (e.g. objects appear when notes play, or move in a certain direction at the tempo). Over time, a trace or lingering effect might show the history of notes (like a fading trail).

- **Avatar or Object-Based Visualization:** Some research projects animate human-like characters or physical analogies to convey music. For instance, an animated conductor or dancer whose movements are driven by the music’s features (tempo, intensity) can give visual rhythm cues. Others have used **synesthetic mappings**, like a *“virtual character whose performance elicits a response”* (movement) to the music ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=and%20map%20them%20to%20the,to%20extract%20affective%20features%2C%20it)), or even facial animations (e.g. the *MusicFace* system that showed an emotive face reacting to the emotional content of music ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=Taylor%2C%20Boulanger%2C%20%26%20Torres%2C%202005%3B,to%20extract%20affective%20features%2C%20it))). While these can be engaging, their accuracy in conveying detailed structure (like exact pitch) might be lower than the direct geometric approaches. They could, however, communicate *emotional tone* or *dynamics* well – for example, a bigger motion for louder sections.

- **Spectral 3D Visuals (Cymatics):** Another angle is to visualize the sound spectrum or waveform in 3D, such as a dynamic landscape or geometry that changes shape with the frequencies. One project, *Baryon*, touts itself as a “3D cymatic music visualizer” simulating the natural geometric patterns of sound (cymatics refers to physical patterns made by sound vibrations). These typically emphasize frequency content (bass vs treble) by shape or position. However, purely spectral approaches may be less informative about musical *structure* (like melody or rhythm) compared to MIDI/event-based visuals. They can be mesmerizing but might require more interpretation – potentially less ideal for conveying precise musical ideas to a deaf audience.

It’s worth noting that **visualizing rhythm** often involves emphasizing periodic events – e.g. using pulsating shapes or moving objects in sync with the beat. A 3D metronome-like object (such as a bouncing ball or flashing light) can anchor the tempo. Some systems use circular arrangements (a 3D spiral or torus) to represent time cycles, which can highlight rhythmic patterns and loops.

### Mapping Musical Elements to Visual Elements  
A successful visualization finds an **expressive visual metaphor** for each important musical element. Here are common mappings used (implicitly or explicitly) in many systems:

- **Pitch (Frequency):** Often mapped to vertical position (echoing the convention of musical notation where higher notes are placed higher). For example, in the MIDITrail image above, notes higher on the piano keyboard are literally higher on the screen ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=sphere%20corresponds%20to%20the%20loudness%2C,the%20timbre%20of%20the%20tone)). Pitch can also be mapped to a radial distance or height in a 3D space. Some visualizers use color for pitch, especially when dealing with scales or harmonic relationships (e.g. a spectrum of colors for the spectrum of audible frequencies). However, using color solely for pitch can be tricky for complex polyphonic music. A piano keyboard graphic or a grid is a helpful reference for pitch mapping.

- **Rhythm (Timing and Duration):** Typically mapped to the horizontal axis or depth (time flow) in a 3D visualization. The movement or the position of notes along one axis indicates when they occur. Duration of notes is often shown by the length of a bar or the time an object remains visible. For instance, a note sustaining might be a long bar or a long trajectory. Rhythm can also be conveyed by periodic motions – e.g. rotating objects that complete a cycle every measure, or pulses of light at each beat. In a 3D scene, one might imagine a rotating **time wheel** or a conveyor belt carrying notes that appear at regular intervals. The key is that the **spacing between notes visually corresponds to the time intervals** in the music.

- **Harmony (Chords, Intervals):** Harmony is essentially multiple pitches sounding together. Visually, one can indicate a chord by grouping the notes – e.g. stacking note objects above each other (since they share the same time). In a 3D piano roll, chords naturally appear as vertical stacks of note blocks at the same time coordinate. Some systems highlight harmonic consonance vs dissonance by color or connecting lines. For example, musicologist Ernst Chladni’s diagrams (historically) showed modal relationships with geometry. In modern visualizations, one might connect chord tones with lines or enclose them in a translucent shape to show they form a chord unit. **Consonant** harmonies could be represented by stable, symmetrically arranged shapes, whereas **dissonant** combinations might be shown with jagged or contrasting visuals (this veers into creative metaphor rather than strict data visualization).

- **Dynamics (Loudness/Volume):** Commonly mapped to the size or brightness of visual elements. A louder note might be a larger sphere or a brighter beam of light, whereas soft notes are smaller or dimmer. In Smith and Williams’ system, loudness was directly mapped to sphere size ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=singers,the%20timbre%20of%20the%20tone)). Brightness, opacity, or even blurriness can also indicate intensity (with loud sounds being vivid and sharp). This mapping helps convey the emotional force of the music visually – e.g., a crescendo (increasing volume) could be seen as swelling shapes or intensifying light.

- **Timbre (Instrument or Sound Color):** Often mapped to **color hue** or shape type. Each instrument or voice can have a dedicated color (e.g. strings are blue, piano is green, drums are red, etc.), allowing viewers to distinguish sources of sound visually ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=data%20to%203D%20space,the%20timbre%20of%20the%20tone)). Alternatively, different geometric primitives could represent different instruments: cones for trumpets, cylinders for piano, etc., though this can become complex. Color is a straightforward way to label instruments, as seen in many MIDI visualizers including MIDITrail (each track/instrument had a different color by default ([MIDI Playback Visualization | University of North Texas](https://digitalstrategy.unt.edu/clear/teaching-resources/accessibility/music-accessibility/midi-playback-visualization.html#:~:text=Many%20different%20options%20MIDI%20realization,help%20further%20augment%20the%20experience))). Additionally, timbral characteristics (like a brighter sound vs a mellow sound) might be reflected in shape brightness or texture.

- **Melodic Contour and Phrases:** Melodies can be accentuated by connecting successive notes of the same instrument with a thin line or trail, so one can *see* the melody’s shape (rising and falling). Some visualizations leave a fading trail behind moving note objects, which helps indicate the *phrase contour* and also serves the purpose of showing recent history (useful for deaf users to catch what just played) ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=were%20implemented%20to%20provide%20awareness,the%20accuracy%20of%20displayed%20information)). This way, even if notes are short, their ghost remains briefly to indicate the line they traced.

- **Tempo and Meter:** The overall tempo could be shown by a fixed visual element like a beating icon or a pulsating border that flashes each beat or measure. For meter (time signature), a subtle grid or division in the timeline can show beat grouping. In 3D, maybe a set of markers or a floor grid every beat could work. If the music is algorithmically generated, tempo might even change over time; the visualization might respond by adjusting the speed of scrolling or the spacing of time markers accordingly.

- **Musical Form/Sections:** For longer pieces, indicating sections (A/B/A form, verses/chorus, etc.) can be done by changing background colors or camera position for each section, or displaying a label when a new section starts. In 3D, one could imagine moving the camera to a new area of the virtual space for a new section, to visually distinguish it. While this is advanced, it can help show structure at the macro-scale (beyond the immediate note-by-note level).

To illustrate, the system by Smith (1997) we discussed earlier encapsulated many of these mappings directly: *notes as colored spheres* (color = timbre/instrument), *vertical position = pitch*, *sphere size = loudness* ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=singers,the%20timbre%20of%20the%20tone)). Time was not explicitly a spatial axis there, but since it was a static mapping for analysis, one could animate those spheres appearing in time for real-time use. The reason such direct mappings are powerful is that they allow even an untrained viewer to make sense of the music: high notes literally look “high,” loud notes look “big,” etc. Deaf and hard-of-hearing individuals, in particular, benefit from these one-to-one metaphors because they translate sound traits into the visual domain faithfully. In fact, researchers found that providing *distinct visual identifiers for pitch and other features* makes the musical experience much more accessible to the hard-of-hearing ([MIDI Playback Visualization | University of North Texas](https://digitalstrategy.unt.edu/clear/teaching-resources/accessibility/music-accessibility/midi-playback-visualization.html#:~:text=accuracy%20before%20suggesting%20them%20to,terms%20of%20distinct%20pitch%20identifiers)).

### Tools and Prototypes for Accessible Music Visualization  
There have been a number of projects aimed at making music visible (and even tangible) to those who cannot hear it. Here we highlight a few, noting how they implement the above techniques:

- **MIDITrail (open-source)** – as mentioned, displays MIDI files in 3D space with piano keyboards and falling color-coded bars. It essentially turns MIDI data into a live guided light show, and has been suggested as a teaching aid for hard-of-hearing students to *“present an impression of musical events as they occur in real time… accessible to the hard of hearing in terms of distinct pitch identifiers”* ([MIDI Playback Visualization | University of North Texas](https://digitalstrategy.unt.edu/clear/teaching-resources/accessibility/music-accessibility/midi-playback-visualization.html#:~:text=accuracy%20before%20suggesting%20them%20to,terms%20of%20distinct%20pitch%20identifiers)). The user can navigate the virtual camera to focus on different aspects (e.g. zoom in on one instrument).

- **VisiSounder prototype** – described in a study for hearing-impaired students, it used animated characters (like frogs and monkeys) hitting percussion instruments in sync with rhythms ([](https://iiisci.org/journal/pdv/sci/pdfs/P958927.pdf#:~:text=Sobieczky%20visualized%20the%20consonance%20of,10)) ([](https://iiisci.org/journal/pdv/sci/pdfs/P958927.pdf#:~:text=Subjects%20ents%20,either%20a)). While somewhat whimsical, it provided clear visual timing cues. Another mode in the same study mapped MIDI notes to 3D graphics directly (as per Smith’s mapping) ([](https://iiisci.org/journal/pdv/sci/pdfs/P958927.pdf#:~:text=Sobieczky%20visualized%20the%20consonance%20of,for%20users%20to%20browse%20and)). They tested these with deaf students to see which helped them perform along with music, concluding that an intuitive visualization can indeed assist timing.

- **AudioLux & Cymatic Lighting** – an open-source project (*AudioLux*) combines software and hardware (LED light strips) to turn music into synchronized light patterns in real time ([
    AudioLux Audio Visualizer | Events | Oregon State University  ](https://events.engineering.oregonstate.edu/expo2023/project/audiolux-audio-visualizer#:~:text=The%20AudioLux%20Audio%20Visualizer%20is,LED%20strip%20in%20real%20time)). Although not a 3D graphics display, it’s a spatial visualization using physical lights around a room. This project was specifically motivated to include deaf individuals in music events by translating audio to visual light shows ([
    AudioLux Audio Visualizer | Events | Oregon State University  ](https://events.engineering.oregonstate.edu/expo2023/project/audiolux-audio-visualizer#:~:text=designed%20to%20make%20speech%20and,LED%20strip%20in%20real%20time)). Similarly, organizations like *Cymaspace* have developed **LED installations** and **“cymatic” visual displays** that respond to live music, creating patterns that can be felt visually (and even with some vibration).

- **ViTune (Visualizer for the Deaf, 2020)** – a research prototype that generates on-screen effects synchronized to music for Deaf users. It’s noted as an *“on-screen visualizer generating effects alongside music”* to enhance musical experience for the Deaf ([ViTune: A Visualizer Tool to Allow the Deaf and Hard of Hearing to ...](https://www.researchgate.net/publication/339212656_ViTune_A_Visualizer_Tool_to_Allow_the_Deaf_and_Hard_of_Hearing_to_See_Music_With_Their_Eyes#:~:text=...%20www.researchgate.net%20%20Paper%20,the%20usage%20of%20an)). Though details are sparse in summary, it likely employs a combination of visual cues (flashes, shapes) to denote beats and melodic movements, possibly even in AR or simple 3D forms.

- **Mobile Apps (BW Dance, etc.)** – There are mobile apps like *BW Dance* that create visualizations and vibrations for the deaf or hard-of-hearing to “feel” the music ([App to turn music into vibrations and visualizations for hearing ...](https://www.audiology-worldnews.com/world-news/market/1962-app-to-turn-music-into-vibrations-and-visualizations-for-hearing-impaired/#:~:text=...%20www.audiology,help%20them%20feel%20the%20music)). The visual component typically might be a dynamic pattern or equalizer on the screen, while the phone’s haptic motor provides a tactile beat. These are 2D and vibro-tactile rather than 3D, but they show the range of multimodal approaches. Another example, not app-based but wearable, is the **SoundShirt** by CuteCircuit – a shirt that converts music into patterns of vibration across the body, allowing deaf users to literally feel each instrument (high violins on the arms, deep drums on the torso, etc.) ([The SoundShirt - CUTECIRCUIT](https://cutecircuit.com/soundshirt/#:~:text=It%20is%20a%20shirt%20which,time%20touch%20%28haptic%29%20sensations)). A comprehensive solution for accessibility might combine such haptic feedback with visual 3D graphics, to engage multiple senses.

In an ideal system aimed at accessibility, one could imagine **MIDI-driven** 3D visuals accompanied by haptic outputs. MIDI is a convenient driving signal since it explicitly provides note events, instruments, velocities (which correlate with dynamics), etc., all of which can feed the visual algorithms directly. Unlike audio-reactive visuals that have to analyze sound to infer these features, a MIDI-driven visual knows the *score* in real time. This means it can be very precise – e.g., it knows exactly when a note starts/stops, what its pitch is, what instrument/channel – and thus can trigger the exact corresponding graphic. This precision is crucial for deaf users to get an accurate depiction of complex music (e.g., distinguishing a fast arpeggio vs a slow melody with accompaniment).

### Toward a Unified Tool: Considerations for Design  
Drawing together the research above, here are some key takeaways that would inform the creation of a tool for algorithmic music generation with 3D visualization:

- **Expressiveness vs Clarity:** The visualization should be expressive enough to represent nuanced musical structure (not just a basic spectrum analyzer), yet remain clear. Using familiar metaphors (like a piano keyboard, or intuitive spatial mappings) helps users orient themselves ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=Although%20this%20music%20display%20is,musical%20features%20from%20live%20performances)). At the same time, creative visuals (like interesting shapes or animations) can enhance engagement – a balance is needed so that aesthetics do not obscure information.

- **Customization:** Different users may have different needs. Deaf users might want maximal information visually (every note clearly marked), whereas hearing users might prefer more abstract visuals that augment the sound. The system could allow turning on/off certain layers (e.g., show/hide pitch names, toggle a minimalist mode). As noted, deaf participants expressed desire to *“customize the information shown”* and check the accuracy ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=were%20implemented%20to%20provide%20awareness,the%20accuracy%20of%20displayed%20information)). This implies a flexible design.

- **Real-Time Performance:** If this tool is coupled with **algorithmic generation** (say a live generative music performance), the visualization must operate in real-time, rendering new events as they are created. Efficiency is key – using MIDI events as input is lightweight and ensures low latency from note generation to visualization. Modern 3D graphics (using game engines or WebGL, etc.) can easily handle dozens of moving objects, so a typical musical texture is manageable.

- **3D Interface & Interaction:** Since it’s 3D, consider allowing the user (or even the performer) to navigate the space – rotating the view or zooming in on certain parts. This interactive element can empower users to explore the music structure (for example, zoom out to see the whole piece structure, or zoom in to see detail of a rapid riff). In VR, a user could literally walk among the notes. Valbom & Marcos’s VR instrument indicated that users appreciated being able to localize sounds in a virtual environment with combined audio-visual cues ([A Survey of Music Visualization Techniques](https://www.audiolabs-erlangen.de/content/05_fau/professor/00_mueller/02_teaching/2024s_sarntal/04_group_VIS/2021_LimaEtAl_MusicVisSurvey_ACM.pdf#:~:text=3D%20music%20and%20sound%20objects%E2%80%9D,localization%20in%20the%20virtual%20environment)). For a deaf user, the visuals *are* the cues, so being able to move around them might aid understanding (seeing a chord from the side to grasp the spacing, etc.).

- **Accessibility Features:** Aside from the core visual mapping, additional features could assist deaf users: e.g., text captions for lyrics (if any), or symbolic indicators of sections (“Chorus” text flashing when a chorus section is playing). Color choices should account for color blindness as well – perhaps offering multiple palettes or using shapes in addition to color. The goal is to ensure the visualization itself doesn’t become a barrier.

In summary, by combining robust **algorithmic music generation** with an **expressive 3D visualization**, one can create a tool that not only composes interesting music but also makes the music’s structure visible to all. Traditional algorithmic techniques (like fractals or Euclidean patterns) can drive the composition in deterministic ways, while AI techniques can add style and complexity; meanwhile, the visualization can map those resulting musical events into a form that is intuitive and captivating. This research foundation can inform an AI coding assistant (such as an interactive development tool) in building such a system – guiding what algorithms to implement for music generation and how to design the visualization component (choice of mappings, graphical libraries, MIDI handling, etc.). Ultimately, the convergence of generative algorithms and visual design serves to **bridge the gap** between sound and sight, enabling a broader audience to experience the richness of algorithmic music.

## References (Cited Works)  

- Chan, Jun Shern. *Algorithmic Composition Tutorial* – discusses using randomness, scales, and fractal L-systems in music generation ([Algorithmic Composition](https://junshern.github.io/algorithmic-music-tutorial/part1.html#:~:text=Fractal%20Music)) ([Algorithmic Composition](https://junshern.github.io/algorithmic-music-tutorial/part1.html#:~:text=well%20convert%20fractals%20to%20music%2C,as%20pitch%2C%20rhythm%2C%20or%20structure)).  
- Wikipedia. "*Algorithmic composition*." (general background, use of fractals/L-systems) ([Algorithmic composition - Wikipedia](https://en.wikipedia.org/wiki/Algorithmic_composition#:~:text=Some%20algorithms%20or%20data%20that,been%20used%20as%20source%20materials)) ([Algorithmic composition - Wikipedia](https://en.wikipedia.org/wiki/Algorithmic_composition#:~:text=Algorithms%20,generating%20techniques%20that)).  
- Harlan J. Brothers. "*The Dawn of Fractal Music*." Medium, 2023 – historical perspective on fractal structures in music ([The Dawn of Fractal Music | Harlan Brothers | Science Spectrum](https://medium.com/science-spectrum/the-dawn-of-fractal-music-f0228a8cb4f#:~:text=It%20turns%20out%20that%20musicians,link%20to%20%E2%80%9Cfundamental%20requirements%E2%80%9D%20above)).  
- Bridges Math Art Archive (2007). "*Fractal Art: Closer to Heaven?*" – defines fractal music and its application to pitches, rhythms, etc..  
- G. T. Toussaint (2005). "*The Euclidean Algorithm Generates Traditional Musical Rhythms*." – introduced the Euclidean rhythm concept ([Euclidean rhythm - Wikipedia](https://en.wikipedia.org/wiki/Euclidean_rhythm#:~:text=The%20Euclidean%20rhythm%20in%20music,number%20of%20beats%20and%20silences)).  
- Miranda, E. R. (1993). "*Cellular Automata Music Composition*." Proceedings of ISEA – CAMUS system mapping CA to music ([](https://isea-archives.siggraph.org/wp-content/uploads/2019/09/1993_Miranda_Cellular_Automata_Music_Composition.pdf#:~:text=This%20paper%20introduces%20an%20experimental,aspects%20through%20an%20example%20composition)) ([](https://isea-archives.siggraph.org/wp-content/uploads/2019/09/1993_Miranda_Cellular_Automata_Music_Composition.pdf#:~:text=that%20music%20researchers%20started%20to,In%20the%20following%20paragraphs%20we)).  
- Magenta TensorFlow (2018). "*Music Transformer: Generating Music with Long-Term Structure*." – describes improved coherence with Transformer vs RNN ([Music Transformer: Generating Music with Long-Term Structure](https://magenta.tensorflow.org/music-transformer#:~:text=Generating%20long%20pieces%20of%20music,performances%20generated%20by%20the%20model)).  
- OpenAI (2019). "*MuseNet*." – a transformer model generating multi-instrument compositions, learned from MIDI data ([MuseNet | OpenAI](https://openai.com/index/musenet/#:~:text=We%E2%80%99ve%20created%20MuseNet%2C%20a%20deep,a%20sequence%2C%20whether%20audio%20or%C2%A0text)).  
- Dong et al. (2017). "*MuseGAN: Multi-track Sequential GAN for Symbolic Music Generation*." – used GANs for multi-instrument music ([MuseGAN: Multi-track Sequential Generative Adversarial Networks ...](https://arxiv.org/abs/1709.06298#:~:text=,GANs)).  
- Nanayakkara et al. (2013). "*Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays*." – discusses mappings (Smith & Williams 1997 spheres) and deaf user preferences ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=singers,the%20timbre%20of%20the%20tone)) ([Enhancing Musical Experience for the Hearing-Impaired Using Visual and Haptic Displays](https://ahlab.org/wp-content/uploads/2021/03/p_nanayakkara_2013_2.pdf#:~:text=icons%20and%20spectrographs%20that%20can,users%20wanted%20to%20be%20able)).  
- University of North Texas, *Music Accessibility*: "*MIDI Playback Visualization*." – recommends MIDITrail’s 3D MIDI visuals for hard-of-hearing students ([MIDI Playback Visualization | University of North Texas](https://digitalstrategy.unt.edu/clear/teaching-resources/accessibility/music-accessibility/midi-playback-visualization.html#:~:text=Many%20different%20options%20MIDI%20realization,help%20further%20augment%20the%20experience)) ([MIDI Playback Visualization | University of North Texas](https://digitalstrategy.unt.edu/clear/teaching-resources/accessibility/music-accessibility/midi-playback-visualization.html#:~:text=accuracy%20before%20suggesting%20them%20to,terms%20of%20distinct%20pitch%20identifiers)).  
- Valbom, Marcos (2005). "*WAVE: Sound and music in an immersive environment*." – VR system with 3D shapes for notes/rhythms ([A Survey of Music Visualization Techniques](https://www.audiolabs-erlangen.de/content/05_fau/professor/00_mueller/02_teaching/2024s_sarntal/04_group_VIS/2021_LimaEtAl_MusicVisSurvey_ACM.pdf#:~:text=%E2%80%9Cthe%20intricate%20body,43)).  
- CHI 2020 Extended Abstracts: Deja et al. "*ViTune: See Music with Your Eyes*." – prototype visualizer for DHH (cited in ([ViTune: A Visualizer Tool to Allow the Deaf and Hard of Hearing to ...](https://www.researchgate.net/publication/339212656_ViTune_A_Visualizer_Tool_to_Allow_the_Deaf_and_Hard_of_Hearing_to_See_Music_With_Their_Eyes#:~:text=...%20www.researchgate.net%20%20Paper%20,the%20usage%20of%20an))).  
- AudioLux (2023). "*Audio Visualizer – Oregon State Univ. Capstone*." – LED strip visualizer for deaf accessibility ([
    AudioLux Audio Visualizer | Events | Oregon State University  ](https://events.engineering.oregonstate.edu/expo2023/project/audiolux-audio-visualizer#:~:text=The%20AudioLux%20Audio%20Visualizer%20is,LED%20strip%20in%20real%20time)).  
- CuteCircuit. "*SoundShirt*." – wearable haptic device for experiencing music through touch ([The SoundShirt - CUTECIRCUIT](https://cutecircuit.com/soundshirt/#:~:text=It%20is%20a%20shirt%20which,time%20touch%20%28haptic%29%20sensations)).
